---
title: "Obtain current connectivity matrices from HYCOMdata"
---


This script downloads currents velecity and direction data from a given geographical area (target archipelago), creates and saves a current connectivity matrix between geographic points (island shores)




```{r}
library(ncdf4)
library(date)
library(lubridate) 
library(curl)
library(raster)
library(rWind)
library(rworldmap) 
library(rworldxtra)
library(shape)
library(tidyverse) 
library(gdistance)
library(fields)
library(scales)
source("Functions.R")
```



# Download HYCOM data



### Function parameters

We set the geographic limits of interest, which consist on a vector with
minlon, maxlon, minlat, maxlat (Longitude values are -180,180), the time interval of the data and the variable sof interest (current velocity and direction)

```{r}

limits_Galapagos <-c(-92,-88.7,-1.8,0.9)

limits_Canaries <-c(-18.795540,-12.643394,26.671542,29.888286)

limits_Azores <-c(-32.205721,-23.441093,36.454755,40.115043)



time<-seq(ymd(paste(1992,10,2, sep="-")), ymd(paste(1995,08,01, sep="-")),
          by="2 days")

vars<-c("water_u", "water_v") #currents direction and speed

```




### Download data

The following chunk downloads HYCOM data according to the parameters set above. Note that the data will be stored in the local repository. *If we dont want to store these files we can modify the get.hycom() function in the Functions.Rmd file to store them as object lists in the global environment.*


```{r}

# Galapagos

for(i in 1:length(time)){
  outputname<-paste("c_output/HYCOM/HYCOM_galapagos/galapagos_", time[i],"_.nc", sep="") #define the name
  #of the output file
  
  get.hycom(limits_Galapagos, time[i], include_latlon = TRUE, filename = outputname,
            depLevels = 1)
}


# Canaries

for(i in 1:length(time)){
  outputname<-paste("c_output/HYCOM/HYCOM_canaries/Canaries_", time[i],"_.nc", sep="") #define the name
  #of the output file
  
  get.hycom(limits_Canaries, time[i], include_latlon = TRUE, filename = outputname, 
            depLevels = 1)
}


#Azores

for(i in 1:length(time)){
  outputname<-paste("c_output/HYCOM/HYCOM_azores/Azores_", time[i],"_.nc", sep="") #define the name
  #of the output file
  
  get.hycom(limits_Azores, time[i], include_latlon = TRUE, filename = outputname, 
            depLevels = 1)
}
```






# Obtain velocity and direction from .NC files


The following chunks use the data downloaded in the code above to obtain velocity and direction rasters of the target geographic area and calculate costs associated with these values. The steps it follows are:

    1.  opening \_.nc files

    2.  (Rotate) function -\> Rotates a raster object that has x
        coordinates (longitude) from 0 to 360, to standard coordinates
        between -180 and 180 degrees.

    3.  (Raster) function -\> to create a raster layer. One for variable
        "water_u", rr_u, and one for variable "water_v", rr_v.

    4.  (atan2)function -\> For Raster\* objects x and y, atan2(y, x)
        returns the angle in radians for the tangent y/x.

    5.  (rad2deg)-\> Transforms between angles in degrees and radians.Sea direction obtained from radiants to degrees Sea speed fromn mathematical formula with u and v

    6.  (stack) -\> A RasterStack to create a collection of RasterLayer
        objects with the same spatial extent and resolution. In this
        case, both Sea_dir and Sea_spe.

Notice that netcdf raster files have the property of including bands. In
particular, these files contain 40 bands. Two of those are u and v
(currents direction and speed). The code below use these bands and save
them to raster stacks.




First we read the .nc files and the .csv with geographic coordinates of islands points we want to connect. 

Notice that the path to read the files may vary depending on when we store the
nc files downloaded with the code above. As default, the path corresponds to pre-created empty files in the repository.

```{r}

# Read files and create a list

files_Galap <- list.files(here::here("c_output/HYCOM/HYCOM_galapagos"),
                          full.names = TRUE, pattern = ".nc") 

files_Canary <- list.files(here::here("c_output/HYCOM/HYCOM_canaries"),
                           full.names = TRUE, pattern = ".nc")


files_Azores <- list.files(here::here("c_output/HYCOM/HYCOM_azores"), 
                           full.names = TRUE, pattern = ".nc") 

 

# Read .csv files with geographic points coordinates created with Qgis

Galapagos_points <- read.csv(here::here("b_data/Data_Galapagos/Galapagos-points.csv"),
                             header=TRUE, sep=",") 

Canaries_points <- read.csv(here::here("b_data/Data_Canaries/CanariesPoints.csv"),
                            header=TRUE, sep=",") 

Azores_points <- read.csv(here::here("b_data/Data_Azores/AzoresPoints.csv"),
                          header=TRUE, sep=",")


#Galapagos_points$id<-NULL
#Canaries_points$id<-NULL
#Azores_points$id<-NULL
```




Calculate costs between geographic points for each downloaded day's data. (steps 1 to 6)




```{r}

cost_list_Galapagos <- list()
cost_list_Canaries <- list()
cost_list_Azores <- list()


cost_list_Galapagos<-calculate_costMatrix(files=files_Galap, geo_points = Galapagos_points)
cost_list_Canaries<-calculate_costMatrix(files=files_Canary, geo_points = Canaries_points)
cost_list_Azores<-calculate_costMatrix(files=files_Azores, geo_points = Azores_points)


```




Replace Inf by Nas

```{r}

Inf2NA <- function(v) replace(v, is.infinite(v), NA)

costlist_clean_Galapagos<-lapply(cost_list_Galapagos, function(d) replace(d, TRUE, sapply(d, Inf2NA)))
costlist_clean_Canary<-lapply(cost_list_Canaries, function(d) replace(d, TRUE, sapply(d, Inf2NA)))
costlist_clean_Azores<-lapply(cost_list_Azores, function(d) replace(d, TRUE, sapply(d, Inf2NA)))

```







# Calculate minimum connectivity matrix


We want to find the minimum sea-dispersal cost (i.e. maximum current connectivity)
between each pair of islands across all the time span of our data. However, we want to avoid picking up an overall single minimum value because we might just be sampling an abnormal connectivity due to either errors in the data or unusual extreme climatic events.

Therefore, we select a small portion of minimum cost values for each point among all the data, and choose the median of that set of minimum values to take it as the minimum connectivity value of the data.

But first we need to check whether the data (each cost value at each day) is distributed normally. Below we analyze the normality of the data conducting shapiro tests on all cost values across the array of matrices (days).



### Analyze the normality of the data


Shapiro test needs, at least, data with two different values. We create an array
with the list of matrices to apply the test across them. The array has the diagonals from all the matrixs filled with 0. So, the diagonal from the first matrix of the array is transformed to '1' values. It doesn't affect the final results, because data from the diagonal is ommited.



```{r}
arr_Galapagos <- array(unlist(costlist_clean_Galapagos), 
                       c(46, 46, length(costlist_clean_Galapagos)))

arr_Canary <- array(unlist(costlist_clean_Canary),
                    c(46, 46, length(costlist_clean_Canary)))

arr_Azores <- array(unlist(costlist_clean_Azores), 
                    c(46, 46, length(costlist_clean_Azores)))


diag(arr_Galapagos[,,1]) <- 1 
diag(arr_Canary[,,1]) <- 1 
diag(arr_Azores[,,1]) <- 1 


shapiro_mat_Galapagos <-apply(arr_Galapagos, c(1, 2), FUN = shapiro.test) 
shapiro_mat_Canaries <-apply(arr_Canary, c(1, 2), FUN = shapiro.test)
shapiro_mat_Azores<-apply(arr_Azores, c(1, 2), FUN = shapiro.test)
```



Shapiro matrix include the results from each Shapiro Test on each matrix position.
On the next code chunk, a p-value is extracted from each Shapiro Test and placed
on a new matrix with the same number of rows and columns (46x46).

Then, the normality of the data is assessed according to Pvalues and the percentage
of data that is normally distributed is calculated



```{r}

pvalue_matrix_Galapagos<-Calculate_Pvalue_matrix(shapiro_matrix = shapiro_mat_Galapagos)
pvalue_matrix_Canaries<-Calculate_Pvalue_matrix(shapiro_matrix = shapiro_mat_Canaries)
pvalue_matrix_Azores<-Calculate_Pvalue_matrix(shapiro_matrix = shapiro_mat_Azores)


normality_matrix_Galapagos <- ifelse(pvalue_matrix_Galapagos > 0.1, "YES", NA)
normality_matrix_Canaries <- ifelse(pvalue_matrix_Canaries > 0.1, "YES", NA)
normality_matrix_Azores <- ifelse(pvalue_matrix_Azores > 0.1, "YES", NA)


percent_normalData_Galapagos <-Calculate_percent_norm_from_normMatrix(normality_matrix=normality_matrix_Galapagos)
percent_normalData_Canaries <-Calculate_percent_norm_from_normMatrix(normality_matrix=normality_matrix_Canaries)
percent_normalData_Azores <-Calculate_percent_norm_from_normMatrix(normality_matrix=normality_matrix_Azores)

```





## Method to find median of the minimum values for non-parametrical data

Median of the minimum 5% of the values

```{r}

median_min_mat_Galapagos<-Calculate_median_minimum5percent(arr_Galapagos)
median_min_mat_Canaries<-Calculate_median_minimum5percent(array =  arr_Canary)
median_min_mat_Azores<-Calculate_median_minimum5percent(arr_Azores)

```

Add names

```{r}
rownames(median_min_mat_Galapagos) = Galapagos_points$id
colnames(median_min_mat_Galapagos) = Galapagos_points$id

rownames(median_min_mat_Canaries) = Canaries_points$id
colnames(median_min_mat_Canaries) = Canaries_points$id

rownames(median_min_mat_Azores) = Azores_points$id
colnames(median_min_mat_Azores) = Azores_points$id
```



```{r}
saveRDS(median_min_mat_Galapagos, "median_min_mat_Galapagos_June24.rds")
median_min_mat_Galapagos <- read_rds("median_min_mat_Galapagos_June24.rds")
```


### Compute matrix minimum connectivity for single points

```{r}

single_point_min_Galap <- compute_mat_min_conn_points(median_min_mat_Galapagos)
single_point_min_Can <- compute_mat_min_conn_points(median_min_mat_Canaries)
single_point_min_Az <- compute_mat_min_conn_points(median_min_mat_Azores)

```

## Plot matrix

```{r}

coord_centroids_gal <- read.csv(here::here("b_data/Data_Galapagos/centroids_network_plot.csv"))

# Load the libraries
library(igraph)
library(ggraph)
library(tidygraph)

# Assuming your adjacency matrix is named 'single_point_min_Galap'
# Convert the matrix to an igraph object
g <- graph_from_adjacency_matrix(single_point_min_Galap, mode = "directed", weighted = TRUE)

# Handle zero weights to avoid division by zero
#E(g)$weight[E(g)$weight == 0] <- NA

# Create a data frame from the igraph object
node_list <- data.frame(name = V(g)$name)

# Merge the coordinates with the node list
node_list <- merge(node_list, coord_centroids_gal, by.x = "name", by.y = "island", all.x = TRUE)

# Ensure no missing coordinates
node_list <- na.omit(node_list)

# Ensure the graph vertices match the coordinates names
V(g)$name <- as.character(V(g)$name)
node_list$name <- as.character(node_list$name)

# Convert the igraph object to a tidygraph object
tg <- as_tbl_graph(g)

# Add coordinates to the tidygraph object
tg <- tg %>%
  activate(nodes) %>%
  left_join(node_list, by = c("name" = "name"))

# Invert the weights: higher values become lower and vice versa
E(tg)$inv_weight <- 1 / E(tg)$weight

# Normalize the inverted weights for better visualization
max_inv_weight <- max(E(tg)$inv_weight, na.rm = TRUE)
E(tg)$norm_inv_weight <- E(tg)$inv_weight / max_inv_weight

# Plot the graph using ggraph with spatial coordinates
ggraph(tg, layout = 'manual', x = V(tg)$long, y = V(tg)$lat) + 
  geom_edge_link(aes(width = norm_inv_weight, edge_alpha = norm_inv_weight), arrow = arrow(length = unit(3, 'mm')), end_cap = circle(3, 'mm')) +
  geom_node_point(size = 5, color = 'blue') +
  geom_node_text(aes(label = name), repel = TRUE, size = 5) +
  scale_edge_width(range = c(0.5, 2)) + # Adjust the range for better visualization
  theme_void() + 
  theme(legend.position = "none")

```


Add colors based on in and outdegree


```{r}
library(ggrepel)

# Calculate in-degree and out-degree based on the inverted weights
V(tg)$in_degree <- strength(tg, mode = "in", weights = E(tg)$inv_weight)
V(tg)$out_degree <- strength(tg, mode = "out", weights = E(tg)$inv_weight)


# Plot the graph using ggraph with spatial coordinates and color nodes based on in-degree
ggraph(tg, layout = 'manual', x = V(tg)$long, y = V(tg)$lat) + 
  geom_edge_link(aes(width = norm_inv_weight, edge_alpha = norm_inv_weight), arrow = arrow(length = unit(3, 'mm')), end_cap = circle(3, 'mm')) +
  geom_node_point(aes(color = in_degree), size = 6) +
  scale_edge_width(range = c(0.5, 2)) + # Adjust the range for better visualization
  scale_color_viridis(name = "In-Degree", option = "turbo") + # Use viridis color scale
 geom_node_text(aes(label = name), repel = TRUE, 
                 nudge_y = 0.1, 
                nudge_x = -0.04, size = 5, segment.size = 0.15) +
  theme_void() + 
  theme(legend.position = "bottom")
```





### Verification of the method and correlation with minimum values



```{r}

#saveRDS(min_matrix_Galapagos, "min_matrix_Galapagos.RDS")

min_matrix_Galapagos <- apply(array(unlist(cost_list_Galapagos), c(46, 46, 4769)), c(1, 2), min)
min_matrix_Canaries <- apply(array(unlist(cost_list_Canaries), c(46, 46, 4769)), c(1, 2), min)
min_matrix_Azores <- apply(array(unlist(cost_list_Azores), c(46, 46, 4769)), c(1, 2), min)

Corr_min_Galapagos <-Correlation_minMatrix_medianMinMatrix(
  median_minimum_matrix=median_min_mat_Galapagos, min_matrix = min_matrix_Galapagos)

Corr_min_Canaries <-Correlation_minMatrix_medianMinMatrix(
  median_minimum_matrix=median_min_mat_Canaries, min_matrix = min_matrix_Canaries)

Corr_min_Azores <-Correlation_minMatrix_medianMinMatrix(
  median_minimum_matrix=median_min_mat_Azores, min_matrix = min_matrix_Azores)


print(Corr_min_Galapagos,Corr_min_Canaries,Corr_min_Azores)
```











# Delete from here onward ----------------------


## Making matrices symmetric and saving as csv



The matrices are not symmetric. The cost from point 1 to 2 is different
than from 2 to 1. To obtain a symmetric distance matrix with minimum
values between each pair of points:



#### Matrix-median of minimum values

```{r}
mat_medianMin_sym_Galapagos<-Make_matrix_medianMin_symmetric(median_min_mat_Galapagos)
mat_medianMin_sym_Canaries<-Make_matrix_medianMin_symmetric(median_min_mat_Canaries)
mat_medianMin_sym_Azores<-Make_matrix_medianMin_symmetric(median_min_mat_Azores)

write.csv(mat_medianMin_sym_Galapagos, "../../Matrices/Galapagos/
          Matrices-Currents_Galap/matrix_median_min.csv", row.names = FALSE)

write.csv(mat_medianMin_sym_Canaries, "matrix_median_min_can.csv", row.names = FALSE)

write.csv(mat_medianMin_sym_Azores, "../../Matrices/Azores/
          Matrices-Currents_Azores/matrix_median_min.csv", row.names = FALSE)

```



#### Matrix of absolute minimum values

```{r}
min_mat_sym_Galapagos<-Make_minMatrix_symmetric(min_matrix_Galapagos)
min_mat_sym_Canaries<-Make_minMatrix_symmetric(min_matrix_Canaries)
min_mat_sym_Azores<-Make_minMatrix_symmetric(min_matrix_Azores)

write.csv(min_mat_sym_Galapagos, "../../Matrices/Galapagos/Matrices-Currents_Galap/
          matrix_min.csv", row.names = FALSE)

write.csv(min_mat_sym_Canaries, "../../Matrices/Galapagos/Matrices-Currents_Galap/
          matrix_min.csv", row.names = FALSE)

write.csv(min_mat_sym_Azores, "../../Matrices/Galapagos/Matrices-Currents_Galap/
          matrix_min.csv", row.names = FALSE)
```





```{r}
library(MASS)

rownames(median_min_mat_Galapagos) = Galapagos_points$id
colnames(median_min_mat_Galapagos) = Galapagos_points$id

write.table(format(median_min_mat_Galapagos, digits=4),file="file.csv",sep = ",", row.names = TRUE, col.names = TRUE)

write.csv(median_min_mat_Galapagos, "median_min_mat_Galapagos.csv")
```






